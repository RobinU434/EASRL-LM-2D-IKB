lr_q: 0.001
init_alpha: 0.01
gamma: 0.98  # has to be zero for imitation learning
batch_size: 32
buffer_limit: 50000
start_buffer_size: 1000
train_iterations: 20
tau: 0.01  # for target network soft update,
target_entropy: -40.0 # for automated alpha update,
lr_alpha: 0.001
n_epochs: 5000
actor:
  type: LatentActor  # also possible (SuperActor LatentActor)
  learning_rate: 0.0005
  learning_mode: 0  # regulates if we fine tune the model on data from rl agent or not
  latent_dim: 8  # only relevant for latent actor
  latent_checkpoint_dir: "results/vae/baseline"  # specify directory where to look for a suiting checkpoint directory <num_joints>_<latent_dim>_<time_stamp>
  architecture: [128, 128]
  activation_function: ReLU


