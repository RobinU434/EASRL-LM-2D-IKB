# Meeting 25.11.2022


## Additional Information to my initial ideas

### First approach

A really big problem is that this approach becomes simply not feasable if the sequence of actions is very long (like millions of actions). The time consumption to evalueate all of these actions is simply to high and NOT practical.

### secodn approach

Problem with this approach: why should a model return more than one action with the propability to apply it. Plan: look for papers with a focus on policy gradient methods and categorical actions.

## Papper: Deep Reinforcement Learning in Large Discrete Action Spaces

The paper proofs my concerns regarding my own ideas with resprect to the time complexity. On the other side they introduce a method where they introduce wolpertinger training. Here the try to cope with extremly large action spaces. In order to do so they encode their actions in a latent space R^n. The policy maps from a state s into this latent space with a^. Almost all of the time the policy doesn't hit a certain encoded action. To chose despite an action the sample the k nearest neighbours (approximate k nearest neighbours -> O(log(n)) ) and evaluate them with Q(s, a^). 

New idea: output from policy are also limits where we have to look for action to evaluate with Q(s, a^) wich satisfy these constraints. But be aware that those limits are not to lazy so that the algrithms tries to evaluate all possible actions -> Punish with L2 loss or with a higher order the amountt of actions to evaluate. 

## Plan

- Find toy example 
- maybe a planning env where one trajectory resembles one action and the action space is the set of all possible actions.exmples: 
	- Quadrocopter trajectories
	- A1 leg trajectories, like with DeepGait paper
	- trajectory planning with trains like flatland 
	- trajectory planning with graphs (path finding, traveling salesman, ...)
	- trajetory planning for people on the side walk / traffic simulation
	- 

