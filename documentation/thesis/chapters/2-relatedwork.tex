\chapter{Related Work}\label{chap:relatedwork}

In the following section we will briefly explain the already existing research regarding Reinforcement Learning together with latent models.

\section{Learning Operational Space Control}

The authors of ``Learning Operational Space Control''\cite{Learning_to_Control_in_Operational_Space}, Jan Peters and Stefan Schaal, present a novel approach for controlling the end-effector of a rigid robot arm. Using operational space control in robots meant to work safely in human environments is challenging due to unmodeled nonlinearities, leading to accuracy reduction and unpredictable behavior.\\
To address this challenge, learning control methods are explored. Traditional learning methods struggle to capture the structured knowledge required for operational space control, such as Jacobians and inertia matrices, which may not always be observable. This paper introduces novel approaches to learning operational space control, focusing on learning the operational space control law directly, similar to an inverse model learning problem. \\
Key insights for this project include the realization that a physically correct solution to the inverse problem with redundant degrees-of-freedom is attainable through piecewise linear learning. Additionally, many operational space controllers can be viewed as constrained optimal control problems. A learning algorithm is formulated to synthesize a globally consistent desired resolution of redundancy while learning the operational space controller, treated as a reinforcement learning problem maximizing an immediate reward.

\section{Motor synergy development in high-performing deep reinforcement learning algorithms}

In their paper ``Motor synergy development in high-performing deep reinforcement learning algorithms''\cite{Motor_Synergy_Learning} the researchers Jiazheng Chai and  Mitsuhiro Hayashibe investigate if the motor synergy concept could also be observed in deep reinforcement learning for robotics. The motor synergy concept states that a sets of actuators e.g. muscles in animals or motors in robots, are controlled by a reduced set of commands with respect to the degrees of freedom resembled by the actuators. They study this concept by training an agent either with SAC or TD3 on several benchmark environments like Half-Cheetah \cite{Half_Cheetah}, and further investigate if it is possible ot reduce the dimensionality of the collected control signal via principal component analysis \cite{PCA} and reconstruct it as close as possible to the original signal.


\section{LASER: Learning Latent Action Space for Efficient Reinforcement Learning}

In the paper ``LASER: Learning Latent Action Space for Efficient Reinforcement Learning'' from Arthur Allshire, Roberto Martín-Martín, Charles Lin, Shawn Manuel, Silvio Savarese and Animesh Gartg from 2021 \cite{LASER} the authors trained jointly a Soft Actor-Critic algorithm and a conditional Variational Autoencoder for robot manipulation. The authors split the problem into two sub-problems. First they lear a mapping $g(o): \mathcal{O} \to \bar{\mathcal{A}}$ from observation space into a latent space and second using a robot controller $f(\bar{a}): \bar{\mathcal{A}} \to \mathcal{A}$ to map their latent signals into actuation commands. Their algorithm LASER is trained as an encoder-decoder model which learns to map manifold of low-level commands to a latent action space. 

% \begin{figure}
%     \centering
%         \includegraphics[width=0.46 \linewidth]{figures/place_holder.png}
%     \caption[LASER workflow]{Workflow of LASER approach. The image was adapted from \cite{LASER}}
%     \label{fig:LASER_workflow}
% \end{figure}
% 
% In \figref{fig:LASER_workflow} we can observer the proposed workflow. Important to note is that the Variational Autoencoder is trained either on data collected by a different policy (offline), like an expert or it is trained on the fly, using data generated by the policy and itself via variational inference. 





\section{CALM: Conditional Adversarial Latent Models for Directable Virtual Characters}

Researchers from NVIDIA  Chen Tessler et.al. have developed Conditional Adversarial Latent Models (CALM)  in the paper ``CALM: Conditional Adversarial Latent Models for Directable Virtual Characters''\cite{CALM} to enable users to direct the behavior of virtual characters in interactive simulations. CALM combines imitation learning with a control policy and motion encoder to create a representation of human motion that is diverse and controllable. The key phases of CALM include:
\begin{enumerate}
    \item low-level training, where it learns a motion encoder and decoder
    \item directionality control, where a high-level policy guides motion direction and style
    \item and inference, where trained models are combined to compose complex character movements using a finite-state machine
\end{enumerate}
This approach allows users to intuitively control virtual characters, making it applicable for interactive applications like video games.