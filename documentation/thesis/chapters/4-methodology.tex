\chapter{Methodology}\label{chap:Methodology}

covers:
research idea
rl-environment
experiments on VAE, Supervised and merged pipeline

\section{Research Idea}

The primary research idea of my thesis is to pursue a transformation from the RL environment action space $\mathcal{A}$ into a lower-dimensional latent action space $\mathcal{A}_L$. This latent action space will align with the latent space between the encoder and decoder of a VAE model or the feature space of a feed-forward-neural network. By doing so, we aim to enable RL agents to effectively explore and learn within a more compact and smoothened action representation.

We propose two potential approaches for achieving this reduction in dimensionality: VAEs and supervised models.

In the VAE approach, a conditional or unconditional generative model is employed to learn a latent representation of the action space. By training the VAE on actions from an expert or on state target combinations to emphasis a solution which is independent from an expert, we aim to capture the underlying structure and patterns within the action space from the RL environment. This latent representation can potentially offer a more concise and informative representation of the actions, encourage more efficient learning and exploration for RL agents.

Alternatively, the supervised model approach involves training a supervised learning model, such as a neural network, to directly transform the a defined lower dimensional latent action into the high-dimensional action space. This transformation is learned based on labeled examples of actions and their corresponding latent representations. In the conducted experiments the lower dimensional-action space is just the desired action outcome. By leveraging supervised learning techniques, we aim to find a mapping between state information and desired action outcome to the RL environment action space, allowing RL agent to operate effectively on a higher level within the reduced-dimensional latent action space..

\section{RL Environment}\label{sec:RL-Environment}

To apply RL in general you need an environment the agent can send actions to and receives feedback as discussed in \secref{sec:RL-Framework}. As previously introduced the key problem we are targeting is inverse kinematics of a robot arm. This problem turn out to be suitable because:
\begin{itemize}
    \item the action space is scalable by simply adding additional joints to the robot arm
    \item it can be simplified into a 2D space with a fast and reliable implementation
    \item it is expandable. You are always able to extend the environment with additional constrains like objects the robot has to navigate around or joint angle constrains.
\end{itemize}

In this section I'm going to present the a novel RL-Environment for bench-marking the performance of different algorithms to solve inverse kinematics for a robot arm with $N$ many joints and subsequently $N$ many segments with lengths $l \in\mathbb{R}_{>0}^N$ in 2D space. For simplicity reasons $l$ is constant with $l = \{ 1 \}^N$.

\subsection{State Space}

The state space $\mathcal{S} \subset \mathbb{R}^{4 + N}$ for this environment consists of three main building blocks. 

\begin{itemize}
    \item \textbf{goal information} $p_{\text{target}, t} \in \mathbb{R}^2$: This vector provides information where to move the end-effector. This position is lies always in for the robot arm, reachable distance. Mathematical speaking: $||p_\text{target}||_2 \leq \sum l$ 
    \item \textbf{state position} $p_{\text{current},t} \in \mathbb{R}^2$: This vector contains the current end-effector position in 2D space and should help the agent to understand where the end-effector is placed and how an action has influenced the current end-effector position. Because the current position is attached to the robot arm: $||p_\text{current}||_2 \leq \sum l$.
    \item \textbf{joint angles} $q_t \in [0, 2\pi)^N$: This vector contains information about the joint angle configuration at time $t$.
\end{itemize}
A state $\mathcal{s}_t \in \mathcal{S}$ at time $t$ has for all $t$ the same composition

\begin{equation}[p]\label{eqn: state}
    \mathcal{s}_t = (p_{\text{target}, t}, p_{\text{current}, t}, q_t)
\end{equation}

To refer to individual parts from index $i$ to $j$ of a state with: $s_{t, (i, j)}$. Therefor we can extract the individual parts with:
\begin{itemize}
    \item $p_{\text{target}, t} = s_{t, (0, 1)}$ 
    \item $p_{\text{current}, t} = s_{t, (2, 3)}$
    \item $q_{t} = s_{t, (4, N + 4)}$
\end{itemize}

\subsection{Action Space}

The action space $\mathcal{A} \subseteq \mathbb{R}^N$ for this particular environment is continuous and contains all possible joint angle configurations for a robot arm with $N$ joints. 

A generated action $\hat{a}$ from the agent is sent to the environment. Inside the environment the incoming action is added on top of the current state angles $q_t$. To ensure the constrains of $q_{t+1} \in [0, 2\pi)^N$ we take the signed remainder of a division by $2\pi$ to write into $q_{t+1}$:
\begin{equation*}
    q_{t+1} = (q_t + \hat{a}) \ \% \ 2\pi
\end{equation*}

Subsequently after updating the state angles the current end-effector position get updated by a forward kinematics call on $q_{t+1}$:
\begin{equation*}
    p_{\text{current}, t} = fk(q_{t+1})
\end{equation*}


\subsection{Reward Function}

The reward function $R: \mathcal{A} \times \mathcal{S} \to \mathbb{R}$ as in \eqref{eqn:Rewardfunction} for the conducted experiments and this environment aims to minimize the distance between the current end-effector position $p_{\text{current}, t}$ and the current target position $p_{\text{target}, t}$. The current end-effector position is calculated by the forward-kinematics function on state-angles $q_t$ plus action $a_t$. The target position is sampled at the beginning of an episode and stays constant throughout the episode until completion or time limit is reached. 

\begin{equation}\label{eqn:Rewardfunction}
    R(s_t, a_t) = ||\text{forward kinematics}(s_{t, (4, \ldots, N + 4)}  + a_t), s_{t, (0 ,1)}||_2
\end{equation}

\section{Dataset Creation}

why do we need a dataset
what kinds of different ways to create a dataset

\subsection{Uniform Sampling}

Vanilla uniform sampling algorithm for sampling from a uniform distributions.
What can we observe:
    radius distribution of 2D forward kinematics is very similar to a half Normal distribution
    with increasing number of joints the std relative the arms reach shrinks
    plot how strong it shrinks
    but we need a uniform distribution wrt. radius and angle. 

\subsection{Expert Guidance}

Dataset creation with expert guidance describes an algorithm:
    1. sample a position in 2D space from a uniform and takes it as the state position
    2. sample initial arm angles from a uniform distribution
    3. solve IK for this position and with the random angles from 2. as start and take the resulting angles as the state angles
    4. sample a second position in 2D as the target position
    5. solve IK for the target position and with the state angles as start and take the difference between resulting angles and state angles as the action.

advantages:
    - we are able to sample target positions and state positions uniformly wrt. angel and radius

disadvantages:
    - we are bound the the solutions an expert is providing
    - it is slower compared to the vanilla uniform sampling algorithm

Plots
    Runtime complexity wrt. number of joints
    expert actions with 2 joints for demonstrating that the actions of the action space cover only a very limited subset of possible actions. Why Two -> because we can just draw a heatmap in two dimensions

\section{Latent Criterion}

In this section I will cover the employed loss functions to train the latent models. 
As discussed in \secref{sec:VAE} we employ the Evidence Lower Bound as the objective function to train Variational Autoencoder. To fit our problem of inverese kinematics we tried out 3 different reconstruction loss function.

In this section we will explain the tangible KL divergence as well as the individual reconstruction loss functions.

\subsection{Kulback Leiber Divergence}

The Kulllback Leiber divergence first pulbished by Solomon Kullback and Richard Leiber  in 1951 \todo{cite: https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-1/On-Information-and-Sufficiency/10.1214/aoms/1177729694.full} is mathematical measure of how a probability distribution $P$ differs from a second distribution $P'$ and is denoted as $D_\text{KL}(P||P')$. Inside the ELBO it functions as a regularization element between the latent distribution $p_\phi(z|x)$ and a target distribution $p(z)$. Throughout this thesis we take the standard normal distribution as the target distribution, $p(z) = \mathcal{N}(0, I)$. Since we implement a parameterized gaussian as the latent distribution due to the nature of continuous input data, $p_\phi(z|x) = \mathcal{N}_\phi(\mu_\phi(x)| \ \Sigma_\phi(x))$. Therefor we can calculate the Kullback Leiber Divergence as:
\begin{align*}
    D_\text{KL}( \mathcal{N}_\phi(\mu_\phi(x)| \Sigma_\phi(x))|| \mathcal{N}(0, I)) &= \frac{1}{2} \left(\mu_\phi(x)^T\mu_\phi(x)  + tr\left(\Sigma_\phi(x)\right) - K - \log{|\Sigma_\phi(x)|} \right)\\
    &= \frac{1}{2} \left(\mu_\phi(x)^T\mu_\phi(x)  + \sum_{i=1}^K\Sigma_\phi(x)_{ii} - K - \log\left(\sum_{i=1}^K\Sigma_\phi(x)_{ii}\right)\right) 
\end{align*}\todo{employ in code and test diffferent KL divergence} \todo{proof in appendix from https://mr-easy.github.io/2020-04-16-kl-divergence-between-2-gaussian-distributions/}
\todo{KL div}

\subsection{Reconstruction Loss}

Inside the Evidence Lower Bound objective, the reconstruction loss minimizes the distance between the input data $x$ and the model output $\hat{x}$. In our experiments we employed three different approaches for the reconstruction loss:


\textbf{Imitation Loss}. The imitation loss is a criterion to minimize the mean squared error between a given label $y$, in this case an action from an expert which results in $y \in \mathbb{R}^N$ and the predicted action $\hat{x} \in \mathbb{R}^N$. It is defined as in \eqref{eqn:Imitation-Loss}.

\begin{equation}[p]\label{eqn:Imitation-Loss}
    \begin{split}
        \mathcal{L}_\text{Imitation}: \mathbb{R}^N \times \mathbb{R}^N & \to \mathbb{R} \\
        y, \hat{x} & \mapsto \frac{1}{N}\sum_{i = 0}^{N-1} (y_i - \hat{x}_i)^2 
    \end{split}
\end{equation}


\textbf{Distance Loss}. This loss function minimizes the distance between a desired position in 2D space as label $p_\text{target}$ and the action outcome of a state action combination. The state information needed for this criterion are only the current robot arm angles $q$. Like before the action is referred to as $\hat{x}$. \todo{wirte forward kinematics algorithm}

\begin{equation}[p]\label{eqn:Distance-Loss}
    \mathcal{L}_\text{Distance}(p_\text{target}, \hat{x}, q) = ||p_\text{target} - \text{forward kinematics}(q + \hat{x})||_2
\end{equation}

This criterion is also used in a standard supervised learning approach where the model predicts actions based on state information.

\textbf{Inverse kinematics Loss}. This criterion as in \eqref{eqn:IK-Loss} is the result of merging distance loss and imitation loss in one loss function as a weighted sum of those two components with weights $w_\text{Imitation}, w_\text{Distance} \in \mathbb{R}$. Inside the code, the loss function can be configured to work also a pure distance loss function without providing any expert action and make it therefor also applicable to the afore mentioned supervised learning approach. 

\begin{equation}\label{eqn:IK-Loss}
    \mathcal{L}_\text{IK}(y, p_\text{target}, \hat{x}, q) = w_\text{Imitation} \cdot \mathcal{L}_\text{Imitation}(y, \hat{x}) + w_\text{Distance} \cdot \mathcal{L}_\text{Distance}(p_\text{target}, \hat{x}, q)
\end{equation}

\section{Learning Variational Autoencoder}

\subsection{}

\section{Learning conditional Variational Autoencoder}


\section{Software}

In the following section we are going to present the developed software stack to train the latent models as well as the reinforcement learning agent.

\subsection{Inverse kinematics Environment}

As presented previously in \secref{sec:RL-Environment} we tackle the problem of inverse kinematics of a robot arm with $N$ many joints in a two dimensional setting. 
The implemented environment

\subsection{Latent Module}

The latent module contains every functionality regarding either a Variational Autoencoder or a simple feed forward supervised regression model.
As a deep learning library we build on pytorch 1.\todo{find out version}. 

You can find out about the functionality either with 



\subsection{Soft actor critc}

