\chapter{Experiments}\label{chap:experiments}

% \input{figures/experiments/figExample}
% \input{figures/experiments/tabAccuracy}


\section{Baseline SAC}

First we would like to have a look how a the plain SAC algorithm, where the actor network directly returns actions for the environment, will perform on the inverse kinematics environment. Further we will describe those kind of experiments as sac baseline experiments. In \tabref{tab:SAC_Baseline_Hyperparameters} we describe the basic hyperparameter used for this line of experiments.
\begin{figure}
    \begin{center}
        \subfloat[Mean reward per step over the last 20 episodes.]{
            \includegraphics[width=0.46 \linewidth]{figures/experiments/sac_baseline_mean_score.png}
            \label{fig:SAC_baseline/reward}
            }
        \hfill
        \subfloat[Mean episode length over the last 20 episodes]{
        \includegraphics[width=0.46 \linewidth]{figures/experiments/sac_baseline_episode_len.png}
            \label{fig:SAC_baseline/episode_len}
            }
    \end{center}
    \caption[SAC baseline experiment results]{SAC baseline experiment results with an increasing number of joints. We can clearly see that the algorithm does not scale well with an increasing number of joints and therefor drops in performance. Each experiment was conducted 10 times and the shaded areas resemble the standard deviation around the mean.}
    \label{fig:SAC_baseline}
\end{figure}

In \figref{fig:SAC_baseline} we plotted the corrected mean reward and the average episode length over the last 20 epochs for different $N \in [2, 5, 10, 15, 20]$. Overall we can observe a decreasing mean of collected reward in \figref{fig:SAC_baseline/reward} while an increasing amount of steps in one episode in \figref{fig:SAC_baseline/episode_len} by increasing the number of joints $N$. In \figref{fig:SAC_baseline/episode_len} the training curves are approaching the maximum step limit of one episode while increasing the number of joints. Additionally we can observe that the standard deviation over multiple experiments is decreasing while increasing the number of joints.

To show the actual behavior of an agent in a standardized setting we also plotted an inference step as in \figref{fig:SAC_baseline_inference_distance} and the distance remaining to target. The standardization \figref{fig:SAC_baseline_inference_trajectory} for one selected experiment for a relative consistent target position at $[-0.5, 0.5] \cdot N$. \\
As expected from the performance statistics in \figref{fig:SAC_baseline} agents with 2 to 5 joints perform much better than agents with 10 or more joints. \\
Interesting to observe is that all agents are minimizing the distance to the target quite fast in their first couple of steps. But after missing the target closely the agent behavior becomes kind of desperate and the end effector moves towards the origin. After increasing the distance by moving towards the origin the distance starts to oscillate as we can see in \figref{fig:SAC_baseline_inference_trajectory/greedy_15} and \figref{fig:SAC_baseline_inference_trajectory/greedy_20} and nevers goes below the threshold of 0.1. \todo{discuss}
\begin{figure}
    \begin{center}
        \subfloat[From experiment 2\textunderscore 1691621262 at checkpoint 5000 ]{
            \includegraphics[width=0.31 \linewidth]{figures/experiments/greedy_inference_baseline_2_1691621262_5000.png}
            \label{fig:SAC_baseline_inference_trajectory/greedy_2}
            }
        \hfill
        \subfloat[From experiment 5\textunderscore 1691624939 at checkpoint 5000. To make things more clear only every 5th robot arm is drawn]{
            \includegraphics[width=0.31 \linewidth]{figures/experiments/greedy_inference_baseline_5_1691624939_5000.png}
            \label{fig:SAC_baseline_inference_trajectory/greedy_5}
            }
        \hfill
        \subfloat[From experiment 10\textunderscore 1691622498 at checkpoint 5000.To make things more clear only every 5th robot arm is drawn]{
            \includegraphics[width=0.31 \linewidth]{figures/experiments/greedy_inference_baseline_10_1691622498_5000.png}
            \label{fig:SAC_baseline_inference_trajectory/greedy_10}
            }
        \\
        \subfloat[From experiment 15\textunderscore 1691619106 at checkpoint 5000. To make things more clear only every 40th robot arm is drawn]{
            \includegraphics[width=0.31 \linewidth]{figures/experiments/greedy_inference_baseline_15_1691619106_5000.png}
            \label{fig:SAC_baseline_inference_trajectory/greedy_15}
            }
        \subfloat[From experiment 20\textunderscore 1691619159 at checkpoint 5000. To make things more clear only every 40th robot arm is drawn]{
            \includegraphics[width=0.31 \linewidth]{figures/experiments/greedy_inference_baseline_20_1691619159_5000.png}
            \label{fig:SAC_baseline_inference_trajectory/greedy_20}
            }
    \end{center}
    \caption[SAC baseline inference]{SAC inference. The trajectory is plotted in red. robot arms are drawn in yellow with the little dots as positions of each joint. Target position and start position are scattered in blue and green. The space an end-effector is able to reach is plotted in grey.}
    \label{fig:SAC_baseline_inference_trajectory}
\end{figure}
\begin{figure}
    \begin{center}
        \subfloat[Distance to target policy from experiment 2\textunderscore 1691621262 at checkpoint 5000 ]{
        \includegraphics[width=0.31 \linewidth]{figures/experiments/Distance_to_target_baseline_2_1691621262_5000.png}
            \label{fig:SAC_baseline_inference/distance_2}
            }
        \hfill
        \subfloat[Distance to target policy from experiment 5\textunderscore 1691624939 at checkpoint 5000 ]{
        \includegraphics[width=0.31 \linewidth]{figures/experiments/Distance_to_target_baseline_5_1691624939_5000.png}
            \label{fig:SAC_baseline_inference/distance_5}
            }
        \hfill
        \subfloat[Distance to target policy from experiment 10\textunderscore 1691622498 at checkpoint 5000 ]{
        \includegraphics[width=0.31 \linewidth]{figures/experiments/Distance_to_target_baseline_10_1691622498_5000.png}
            \label{fig:SAC_baseline_inference/distance_10}
            }
        \\
        \subfloat[Distance to target policy from experiment 15\textunderscore 1691619106 at checkpoint 5000 ]{
        \includegraphics[width=0.31 \linewidth]{figures/experiments/Distance_to_target_baseline_15_1691619106_5000.png}
            \label{fig:SAC_baseline_inference/distance_15}
            }
        \subfloat[Distance to target policy from experiment 20\textunderscore 1691619159 at checkpoint 5000 ]{
        \includegraphics[width=0.31 \linewidth]{figures/experiments/Distance_to_target_baseline_20_1691619159_5000.png}
            \label{fig:SAC_baseline_inference/distance_20}
            }
    \end{center}
    \caption[SAC baseline inference]{Distance to target for each action outcome. The target is consistent at $[-0.5, 0.5] \cdot N$. The threshold of 0.1 at which an episode is concluded successfully is drawn in red.} 
    \label{fig:SAC_baseline_inference_distance}
\end{figure}
Another interesting detail to observe is the strategy an agent is following to reach the target position. Instead of moving first closely to a straight line towards the target position the agent is moving its end-effector closely to the outer limit of its reach. As we have seen in section \secref{sec:vanilla_sampling} randomly having an end-effector position far away from the origin is less probable compared to a position close to the origin. 

\section{VAE}

In the following section I will present the results of our experiments with the Variational-Autoencoder to encode and decode actions from the environment but also learn to solve inverse kinematics by minimizing the presented distance loss form \eqref{eqn:Distance-Loss}. 

\subsection{Distance Loss}

Coming up are the training results for different latent dimensions but only with the distance loss enabled which means \texttt{dataset target mode = POSITION}.\\ 
Because we only leverage the usage of the distance loss we don't need to encode and decode an action from an inverse kinematics solver, we just need to encode the target position where we want to go plus the conditional information.

In \figref{fig:VAE_latent} we want to have a closer look into the reconstruction loss and kl loss. While increasing the number of joints with $N \in [2, 5, 10, 15]$ we are also increasing the level of complexity the network has to master to come up with  suitable action which leads to a low distance loss. The increasing level of complexity is quite noticeable in both the kl loss and reconstruction loss. If we first look into the kl loss, all curves are generally shaped in the same way. First a drop before rising with a little overshoot and finally approaching the finial value asymptotically. While increasing $N$ we can see that the first local minimum shifts to the right, the upwards slope becomes slacker and the overshoot diminishes. This behavior confirms the the theory of increasing complexity while increasing the number of joints.

We can find another indicator by analyzing the reconstruction loss in \figref{fig:VAE_latent} (a) to (c). For all joints it is very good to see that the learning curves are shaped similar but approaching different final performances. For those experiments of $N \in [2, 5]$ we get a reconstruction loss $< 0.02$ independent of the latent dimension. But if we turn towards $N = 10$ we can clearly see that the latent dimension does make a significant difference between finding a solution, with a latent dimension of four, or failing with a latent dimension of eight.
\begin{figure}
    \begin{center}
        \subfloat[mean reconstruction loss over the las 20 epochs for \textbf{latent dimension = 2}]{
            \includegraphics[width=0.31 \linewidth]{figures/experiments/vae_comparison_latent_2_reconstruction_loss.png}
            \label{fig:VAE_latent/reconstruction_2}
            }
        \hfill
        \subfloat[mean reconstruction loss over the las 20 epochs for \textbf{latent dimension = 4}]{
            \includegraphics[width=0.31 \linewidth]{figures/experiments/vae_comparison_latent_4_reconstruction_loss.png}
            \label{fig:VAE_latent/reconstruction_4}
            }
        \hfill
        \subfloat[mean reconstruction loss over the las 20 epochs for \textbf{latent dimension = 8}]{
            \includegraphics[width=0.31 \linewidth]{figures/experiments/vae_comparison_latent_8_reconstruction_loss.png}
            \label{fig:VAE_latent/reconstruction_8}
            }
        \\
        \subfloat[mean KL-loss over the las 20 epochs for \textbf{latent dimension = 2}]{
        \includegraphics[width=0.31 \linewidth]{figures/experiments/vae_comparison_latent_2_kl_loss.png}
            \label{fig:VAE_latent/kl_2}
            }
        \hfill
        \subfloat[mean KL-loss over the las 20 epochs for \textbf{latent dimension = 4}]{
        \includegraphics[width=0.31 \linewidth]{figures/experiments/vae_comparison_latent_4_kl_loss.png}
            \label{fig:VAE_latent/kl_4}
            }
        \hfill
        \subfloat[mean KL-loss over the las 20 epochs for \textbf{latent dimension = 8}]{
        \includegraphics[width=0.31 \linewidth]{figures/experiments/vae_comparison_latent_8_kl_loss.png}
            \label{fig:VAE_latent/kl_8}
            }
    \end{center}
    \caption[VAE validation results, only distance loss and latent = 4]{VAE validation results over different amounts of joints and with a latent dimension of 4. Each experiment was conducted 10 times with different random seeds. The solid curve is the average over those 10 experiments and the color shaded area resembles the standard deviation. Notice that the y axis in bot plots is in log scale.}
    \label{fig:VAE_latent}
\end{figure}
\begin{figure}
    \begin{center}
        \subfloat[$N = 2$]{
            \includegraphics[width=0.23 \linewidth]{figures/experiments/vae_comparison_2_latent_2_4_8_reconstruction_loss.png}
            \label{fig:VAE_latent_comparison_reconstruction_loss/2}
            }
        \hfill
        \subfloat[$N = 5$]{
        \includegraphics[width=0.23 \linewidth]{figures/experiments/vae_comparison_5_latent_2_4_8_reconstruction_loss.png}
            \label{fig:VAE_latent_comparison_reconstruction_loss/5}
            }
        \hfill
        \subfloat[$N = 10$]{
            \includegraphics[width=0.23 \linewidth]{figures/experiments/vae_comparison_10_latent_2_4_8_reconstruction_loss.png}
            \label{fig:VAE_latent_comparison_reconstruction_loss/10}
            }
        \hfill
        \subfloat[$N = 15$]{
        \includegraphics[width=0.23 \linewidth]{figures/experiments/vae_comparison_15_latent_2_4_8_reconstruction_loss.png}
            \label{fig:VAE_latent_comparison_reconstruction_loss/15}
            }
    \end{center}
    \caption[VAE latent dimension comparison on reconstruction loss]{Comparison on the VAE latent dimension based on validation results over different amounts of joints. Each experiment was conducted 10 times with different random seeds. The solid curve is the average over those 10 experiments and the color shaded area resembles the standard deviation. Notice that the y axis in bot plots is in log scale. In the legend each label is structured as \texttt{<$N$>\textunderscore<latent dimension>}}
    \label{fig:VAE_latent_comparison_reconstruction_loss}
\end{figure}

By observing the individual number of joints in each plot of \figref{fig:VAE_latent_comparison_reconstruction_loss} form (a) to (d) we can observe that the latent dimension does make a difference in the final reconstruction loss performances of the trained models. Starting with two joints there is no significant difference between the latent dimensions of two to eight. Continuing with 5 joints we can observe no significant difference in the finial performance but a slight difference with respect of how fast the models are converging. The experiments on a latent dimension of four and eight are very similar while the standard deviation for a latent dimension equal to two increases around 600 epochs indicating that some experiments have slight troubles to converge. \\
A significant difference in final performance can be observed with $N = 10$. Here the best performance is returned by experiments with a latent dimension of four closely followed by two. Comparing two and four in \figref{fig:VAE_latent_comparison_reconstruction_loss/10} shows that four is much more stable and faster in learning the problem as two. Last ranked are the experiments for a latent dimension of eight. Those experiments are by far not on the same level as with a latent dimension of two and four. A deeper look into the inference shows that the model fails to come up with an action to reach the given target position.\\
In \figref{fig:VAE_latent_comparison_reconstruction_loss/15} a performance difference is noticeable with a latent dimension of two as best and eight as worst. But since the best performance of the experiments with a latent dimension of two is on the same level as eight in \figref{fig:VAE_latent_comparison_reconstruction_loss/10} we can conclude that the the VAE fails at $N = 15$ with the present hyperparameter as in \tabref{tab:VAE_Hyperparameters}.

\subsection{Distance Loss plus Imitation Loss}

In this section we are going to present the results of training the VAE with the imitation loss to pre-calculated solutions from \algoref{alg:CCD}. The important change in the config file is setting \texttt{dataset target mode = ACTION} to tell the dataset to also calculate action from ccd.  

Because of time limitation we applied this loss function only on a setting with a latent dimension of four since this was the best latent dimension fitted only with the distance loss. 

All in all the distance loss functions in \figref{fig:VAE_imitation/distance_loss} yield almost the same examples as the distance loss functions in \figref{fig:VAE_latent/reconstruction_4}.\\
Additional to the distance loss we have got also an imitation loss between the computed action from  CCD and the action yielded from form the neural Network in \figref{fig:VAE_imitation/imitation_loss}. Here it is interesting to note that the imitation loss is increasing alongside $N$ but not in the same scale. Where the differences between the imitation losses are becoming smaller for bigger $N$. Another thing that is particular interesting is the convergence behavior of each curve. They all hav a huge drop in the beginning but stay on their level for the rest of the training as if the have reached their optimal value. \\
Finally looking at the KL-divergence we do can note an almost indistinguishable plot form \figref{fig:VAE_latent/kl_4}.

\begin{figure}
    \begin{center}
        \subfloat[Distance loss]{
        \includegraphics[width=0.31 \linewidth]{figures/experiments/vae_comparison_imitation_[2, 5, 10, 15]_latent_4 4 4 4_distance_loss.png}
            \label{fig:VAE_imitation/distance_loss}
            }
        \hfill
        \subfloat[Imitation loss]{
        \includegraphics[width=0.31 \linewidth]{figures/experiments/vae_comparison_imitation_[2, 5, 10, 15]_latent_4 4 4 4_imitation_loss.png}
            \label{fig:VAE_imitation/imitation_loss}
            }
        \hfill
        \subfloat[KL diveregence from standard normal distribution]{
        \includegraphics[width=0.31 \linewidth]{figures/experiments/vae_comparison_imitation_[2, 5, 10, 15]_latent_4 4 4 4_kl_loss.png}
            \label{fig:VAE_imitation/kl_loss}
            }
    \end{center}
    \caption[VAE validation results with imitation loss]{Results on the validation dataset while training a VAE with a latent dimension of 4 but incorporating an imitation loss weighted with 0.01. Results are collected from ten different runs and with a solid mean curve and a shaded standard deviation area. $N \in [2, 5, 10, 15]$} 
    \label{fig:VAE_imitation}
\end{figure}
\iffalse

\subsection{Pure Actions}

In every state of the sequential decision making process there are multiple actions available to go from the current state $s_t$ to the next state $s_{t+1}$ where the arm end position is at the goal position. To gain a dataset that contains such actions I sampled the robot arm angles in $s_t$ from a uniform distribution $\mathcal{U}_{[0, 2\pi)}$ and applied the CCD algorithm to get access to the action that leads from $s_t$ to $s_{t+1}$ with the robot arm end position near the goal state. \\

The results of the training process are shown in \figref{}

\subsection{Conditioning on States}

After watching the pure action encoding and decoding fail in the all important reconstruction loss. I got inspired by the paper \todo{cite LASER} and rerolled the experiments with an conditional Variational-Autoencoder where the conditional information is the information from $s_t$. 
The results of the training process are shown in \figref{}

We can observe that the test reconstruction loss is much lower that before and the kl loss

\begin{table}[]
    \centering
    \begin{tabular}{l|l|r|r}
         n & latent dim & r loss & kl loss\\
         2 & 2 & & \\
         2 & 1 & & \\
         5 & 5 & & \\
         5 & 4 & & \\
         5 & 3 & & \\
         5 & 2 & & \\
         5 & 1 & & \\
         10 & 2 & & \\
         10 & 2 & & \\
         10 & 2 & & \\
         10 & 2 & & \\
    \end{tabular}
    \caption{n is the number of joints, latent dimension is the size of the dimension the action is compressed to, r loss is the test reconstruction loss I used the MSE as the metric for the reconstruction loss, kl loss is the Kullback Leiber diveregence from the standard normal distribution on the test dataset}
    \label{tab:CVAE results}
\end{table}
\todo{Concering the table: make a lot of experiments so I can get an std ?, or should I just say that the training turns out to be not as stable as it should be in comparison to an VAE on MNIST and that we sampled runs until we got a good result?}

\subsection{Fitting Random Noise}

In this section I will present the results on how we tried to encode and decode actions sampled from a parameterized distributuion.
The idea is that in every state the agent can choose an action from $[-1, 1]^n$. Therefor I sampled a dataset independent and identically distributed from $\mathcal{U}^n_{[-1, 1]}$. this could be describe of trying to fit random noise.
The results for 800 epochs are shown in \figref{}.

\todo{description?}

\fi

\section{SAC with VAE}

In this section we are going to talk about the experimental results of combining the variational autoencoder with soft actor critic. As we know from \chapref{chap:Methodology} we only use the decoder of the trained variational Autoencoder with input from the corresponding state and a latent action directly sampled from the parameterized distribution from the actor network. 
\begin{table}
    \label{tab:VAE_checkpoints_SAC}
    \begin{center}
        \begin{tabular}{ l | c  c | c  c | c  c}
        \textbf{$N$} & \multicolumn{6}{c}{latent dimension} \\
        \hline
        & \multicolumn{2}{c |}{2} & \multicolumn{2}{c |}{4} & \multicolumn{2}{c}{8} \\
        & random seed & epoch & random seed & epoch & random seed & epoch \\
        \hline
        2   & 1693105015  & 4950 & 1692461245   & 4890 & 1691608881   & 1480 \\
        5   & 1693096269  & 4495 & 1692476369   & 4795 & 1691606505   & 1290 \\
        10  & 1693098415  & 3615 & 1691627455   & 2535 & 1691608513   & 2485  \\
        15  & 1693103204  & 4815 & 1691628295   & 3845 & 1691618374   & 4760  \\
        \end{tabular}
    \end{center}
    \caption[Used VAE checkpoints for SAC]{Used checkpoints to train SAC with decoder from VAE. All experiments can be found in \texttt{results/vae/<$N$>\textunderscore<latent dimension>\textunderscore <random seed>/VAE\textunderscore<epoch>*.pt}}
\end{table}

As we have seen in the previous section we carried out VAE experiments with three different latent space dimensions. Accordingly we trained ten SAC runs for each latent space size and number of joints with $N \in [2, 5, 10, 15]$. We used the best VAE checkpoint available according to the overall validation loss. The detailed listing whoch checkpoints where actually used can be found in \tabref{tab:VAE_checkpoints_SAC}. 

\begin{figure}
    \begin{center}
        \subfloat[mean episode reward per step averaged over the last 20 episodes. ]{
            \includegraphics[width=0.46 \linewidth]{figures/experiments/sac_latent_actor_4_mean_score.png}
            \label{fig:SAC_latent_4/reward}
            }
        \hfill
        \subfloat[mean episode length avergaed over the last 20 episodes]{
        \includegraphics[width=0.46 \linewidth]{figures/experiments/sac_latent_actor_4_episode_len.png}
            \label{fig:SAC_latent_4/episode_len}
            }
    \end{center}
    \caption[SAC + VAE on latent dim = 4]{SAC + decoder form VAE with \textbf{latent dimension = 4}. Each experiment was conducted 10 times with different random seeds. The solid strong line is the mean over those 10 experiments. The color shaded area covering the mean is the standard deviation around the mean.}
    \label{fig:SAC_latent_4}
\end{figure}


\begin{figure}
    \begin{center}
        \subfloat[mean episode reward per step averaged over the last 20 episodes. ]{
            \includegraphics[width=0.46 \linewidth]{figures/experiments/sac_latent_actor_8_mean_score.png}
            \label{fig:SAC_latent_8/reward}
            }
        \hfill
        \subfloat[mean episode length avergaed over the last 20 episodes]{
        \includegraphics[width=0.46 \linewidth]{figures/experiments/sac_latent_actor_8_episode_len.png}
            \label{fig:SAC_latent_8/episode_len}
            }
    \end{center}
    \caption[SAC + VAE on latent dim = 8]{SAC + decoder form VAE with \textbf{latent dimension = 8}. Each experiment was conducted 10 times with different random seeds. The solid strong line is the mean over those 10 experiments. The color shaded area covering the mean is the standard deviation around the mean.}
    \label{fig:SAC_latent_8}
\end{figure}
\begin{figure}
    \begin{center}
        \subfloat[$N = 2$]{
            \includegraphics[width=0.23 \linewidth]{figures/experiments/sac_latent_comp_2_vae_mean_score.png}
            \label{fig:SAC_latent_comparison_mean_score/2}
            }
        \hfill
        \subfloat[$N = 5$]{
        \includegraphics[width=0.23 \linewidth]{figures/experiments/sac_latent_comp_5_vae_mean_score.png}
            \label{fig:SAC_latent_comparison_mean_score/5}
            }
        \hfill    
        \subfloat[$N = 10$]{
            \includegraphics[width=0.23 \linewidth]{figures/experiments/sac_latent_comp_10_vae_mean_score.png}
            \label{fig:SAC_latent_comparison_mean_score/10}
            }
        \hfill
        \subfloat[$N = 15$]{
        \includegraphics[width=0.23 \linewidth]{figures/experiments/sac_latent_comp_15_vae_mean_score.png}
            \label{fig:SAC_latent_comparison_mean_score/15}}
    \end{center}
    \caption[SAC + VAE latent dimension comparison]{Shown are the collected mean scores over the the last 20 episodes per step, compared different latent dimensions and the baseline. Solid line is the mean over 10 experiments and the color shaded area is the corresponding standard deviation. Higher means better with 0 as the maximum. The colored dotted lines mark an end of an experiment.}
    \label{fig:SAC_latent_comparison_mean_score}
\end{figure}

In \figref{fig:SAC_latent_comparison_mean_score} we can observe what difference the choice of latent dimension has on the performance of the Soft Actor Critic algorithm. Quite obviously to see is that the the choice of latent dimension really starts to matter after 5 joints. Where at 10 joints 2 and 4 succeed but 8 doesn't. This is clearly related to the performance of the VAE during its training. There we were also able to see that the latent dimension has an impact of the reconstruction loss performance of the VAE. Therefor it is no surprise to see that the best VAE checkpoints on 15 joints and the best VAE checkpoint on 10 joints with a latent dimension of 8 fails for the SAC + VAE setting. 

By having a closer look into the difference between the collected mean score we can see that for 2 joints all mean curves, the solid ones are packet closely together where at 5 joints we can start to see a slight decrease in performance at 1000 epochs continuing up to 5000 epochs, between the experiments done with a latent dimension of 2 and the others.\\
This decrease is even more noticeable at 10 joints where is now a gap between the experiments done with a latent dimension of 4 and the baseline experiments too. \todo{set in perspective of VAE performance}
\begin{figure}
    \begin{center}
        \subfloat[$N = 2$]{
            \includegraphics[width=0.23 \linewidth]{figures/experiments/sac_latent_comp_2_vae_episode_len.png}
            \label{fig:SAC_latent_comparison_episode_len/2}
            }
        \hfill
        \subfloat[$N = 5$]{
        \includegraphics[width=0.23 \linewidth]{figures/experiments/sac_latent_comp_5_vae_episode_len.png}
            \label{fig:SAC_latent_comparison_episode_len/5}
            }
        \hfill    
        \subfloat[$N = 10$]{
            \includegraphics[width=0.23 \linewidth]{figures/experiments/sac_latent_comp_10_vae_episode_len.png}
            \label{fig:SAC_latent_comparison_episode_len/10}
            }
        \hfill
        \subfloat[$N = 15$]{
        \includegraphics[width=0.23 \linewidth]{figures/experiments/sac_latent_comp_15_vae_episode_len.png}
            \label{fig:SAC_latent_comparison_episode_len/15}
            }
    \end{center}
    \caption[SAC + VAE latent dimension comparison]{Shown are the mean episode length over the the last 20 episodes, compared different latent dimensions and the baseline. Solid line is the mean over 10 experiments and the color shaded area is the corresponding standard deviation. Lower means better with 1 as the minimum and 400 as the maximum.}
    \label{fig:SAC_latent_comparison_episode_len}
\end{figure}

In \figref{fig:SAC_latent_comparison_episode_len} we can observe the mean episode length for different $N \in [2, 5, 10, 15]$. Starting at two joints we can see not much of a difference but at five joints we can observe slight differnces between different latent dimensions. First up is the latent dimension of 2. At this value the experiments perform as in \figref{fig:SAC_latent_comparison_mean_score/5} worst. \todo{discuss why}.\\
Moving on to a latent dimension of four, those experiments perform best in terms of mean episode length and even undercut the baseline experiments although they have not performed as good in terms of the mean score per step in \figref{fig:SAC_latent_comparison_mean_score/5}. Finally those experiments with a latent dimension of eight are performing almost on the same level as the baseline experiments but not as good as the experiments with a latent dimension of 4. \\
In \figref{fig:SAC_latent_comparison_mean_score/10} we can see wide variety of best performances. The best performing setting are the baseline experiments followed by the experiments with a latent dimension of four. The next best performance is coming from a latent dimension equal to two and last ranked are the experiments with a latent dimension of eight. As we have mentioned earlier we suspect this drop in performance caused by the quality the decoder gets hand over to the Soft Actor Critic. \\
Note that although the maximum number of steps per episode was set to 400 the standard deviation of the experiment with a latent dimension of eight seam to surpass this threshold which is with a closer look into the actual curves clearly not the case. \todo{insert plot in appendix}. \\ 
On the performed experiments with 15 joints we can see that none of the latent experiments is able to match the performance of the baseline experiments with even a couple of experiments with a latent dimension of four or eight aborting early. As it turns out this abort is caused by abnormally high values for alpha loss as shown in \figref{fig:alpha_loss_15} resulting in \texttt{nan} values in update steps. A positive alpha loss translates to a an increase of alpha translates to an increase in exploration for the policy, by encouraging a higher standard deviation for the parameterized distribution returned by the actor network. 

As a result of the previous observations we can see the latent dimension as an additional hyperparameter to optimize. Because setting it to high could lead the extremely high variance in the Soft Actor Critic task as shown in \figref{fig:SAC_latent_comparison/10} and also

\section{Supervised}

For the supervised actions we only relied on state information as the input to predict an action to reach from the current state the given target position.  Since this approach is very similar to the Variational Autoencoder setting we are also able to apply the afore mentioned Inverse Kinematics Loss with distance and imitation loss as in \eqref{eqn:IK-Loss}.

The results of the supervised experiments are divided into two parts. The first is only applying the distance loss while the second one is incorporating also an imitation loss with respect ot pre computed actions from the CCD solver.

\subsection{Distance Loss}

The distance loss results over different amount of joints in \figref{fig:supervised_distance} are very similar compared to \figref{fig:VAE_latent} (a) to (c). Parallel to \figref{fig:VAE_latent} we can also observe an increase in loss while increasing $N$. Different to the exoeriments on the VAE is the standard deviation between the experiments. The supervised experiments show a smaller standard deviation across the whole training period compared to the VAE experiments.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.46 \linewidth]{figures/experiments/supervised_2_distance_loss.png}
    \end{center}
    \caption[Supervised Distance Loss]{Distance loss for supervised experiments over different $N \in [2, 5, 10, 15, 20]$. For each $N$ we conducted 10 experiments. }
    \label{fig:supervised_distance}
\end{figure}

\subsection{Distance + Imitation Loss}

For this series of experiments we also want to emphasis solutions close to solutions computed by the CCD by setting the dataset mode to \texttt{ACTION} and setting the imitation loss weight to 0.01. Unlike as in the previous section the distance loss differs a lot from \figref{fig:supervised_distance}. We do not see a steady increase in loss over different numbers of joints we do see vastly different levels of final distance loss performances starting with two joints at around 1.1 and ending with 15 joints at XXX.
A bit more interesting are the results on the imitation loss. Similar to \figref{fig:VAE_imitation/imitation_loss} all curves have a drop in the first couple of epochs but are constant for the rest of the training. Comparing those curves with respect to the number of joints we observed that all mean curves have vastly different levels of convergence which seems not to be linear correlated with $N$. Compared to the experiments in \figref{fig:VAE_imitation/imitation_loss} all imitation losses are higher higher. 

\begin{figure}
    \begin{center}
        \subfloat[Distance Loss]{
            \includegraphics[width=0.46 \linewidth]{figures/experiments/supervised_imitation_2_latent_4 4 4 4_distance_loss.png}
            \label{fig:supervised_imitation/distance_loss}
            }
        \hfill
        \subfloat[Imitation Loss]{
        \includegraphics[width=0.46 \linewidth]{figures/experiments/supervised_imitation_2_latent_4 4 4 4_imitation_loss.png}
            \label{fig:supervised_imitation/imitation_loss}
            }
    \end{center}
    \caption[Supervised Distance and Imitation Loss]{Validation results for supervised experiments on distance and imitation loss. }
    \label{fig:supervised_imitation}
\end{figure}


\section{SAC with Supervised}


\begin{figure}
    \begin{center}
        \subfloat[Distance Loss]{
            \includegraphics[width=0.46 \linewidth]{figures/experiments/}
            \label{fig:supervised_imitation/distance_loss}
            }
        \hfill
        \subfloat[Imitation Loss]{
        \includegraphics[width=0.46 \linewidth]{figures/experiments/supervised_imitation_2_latent_4 4 4 4_imitation_loss.png}
            \label{fig:supervised_imitation/imitation_loss}
            }
    \end{center}
    \caption[Supervised Distance and Imitation Loss]{Validation results for supervised experiments on distance and imitation loss. }
    \label{fig:supervised_imitation}
\end{figure}

