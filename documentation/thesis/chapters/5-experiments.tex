\chapter{Experiments}\label{chap:experiments}

In this chapter we are going to present the experimental results. We focus hereby on two main sections: ``Training Latent Models'' and ``Reinforcement Learning''. In the first section we will cover all results on training Latent Models including experiments with the Variational Autoencoder and Supervised model trained with different loss functions as introduced in \chapref{chap:Methodology}. \\
The second section ``Reinforcement Learning'' is going to present the results on how the Soft Actor-Critic handles the introduced 2D inverse kinematics environment and how the fusion with the pre-trained latent models affects the training


\section{Training of Latent Models}

In this section we will present the results on the conducted experiments how to train a Variational Autoencoder and supervised model. We train both models with two distinct loss functions: The distance loss and a weighted sum of the distance loss and imitation loss. 

\subsection{Variational Autoencoder}

The outcomes of the experiments with the Variational-Autoencoder to encode and decode actions from the environment but also learn to solve inverse kinematics by minimizing the presented distance loss form \eqref{eqn:Distance-Loss} that were conducted will be presented in this section. Within the section we are covering two loss function setups. The first setup we are going to concentrate on only uses the distance loss while the second setup incorporates also an imitation loss.

\subsubsection{Distance Loss}

Coming up are the training results for different latent dimensions but only with the distance loss enabled which means \texttt{dataset target mode = POSITION}.\\ 
Because we only leverage the usage of the distance loss we do not need to encode and decode an action from an inverse kinematics solver, we just need to encode target information where we want to go plus state information as conditional information.
\begin{figure}[h]
    \begin{center}
        \subfloat[mean reconstruction loss over the las 20 epochs for \textbf{latent dimension = 2}]{
            \includegraphics[width=0.31 \linewidth]{figures/experiments/vae_comparison_latent_2_reconstruction_loss.png}
            \label{fig:VAE_latent/reconstruction_2}
            }
        \hfill
        \subfloat[mean reconstruction loss over the las 20 epochs for \textbf{latent dimension = 4}]{
            \includegraphics[width=0.31 \linewidth]{figures/experiments/vae_comparison_latent_4_reconstruction_loss.png}
            \label{fig:VAE_latent/reconstruction_4}
            }
        \hfill
        \subfloat[mean reconstruction loss over the las 20 epochs for \textbf{latent dimension = 8}]{
            \includegraphics[width=0.31 \linewidth]{figures/experiments/vae_comparison_latent_8_reconstruction_loss.png}
            \label{fig:VAE_latent/reconstruction_8}
            }
        \\
        \subfloat[mean KL-loss over the las 20 epochs for \textbf{latent dimension = 2}]{
        \includegraphics[width=0.31 \linewidth]{figures/experiments/vae_comparison_latent_2_kl_loss.png}
            \label{fig:VAE_latent/kl_2}
            }
        \hfill
        \subfloat[mean KL-loss over the las 20 epochs for \textbf{latent dimension = 4}]{
        \includegraphics[width=0.31 \linewidth]{figures/experiments/vae_comparison_latent_4_kl_loss.png}
            \label{fig:VAE_latent/kl_4}
            }
        \hfill
        \subfloat[mean KL-loss over the las 20 epochs for \textbf{latent dimension = 8}]{
        \includegraphics[width=0.31 \linewidth]{figures/experiments/vae_comparison_latent_8_kl_loss.png}
            \label{fig:VAE_latent/kl_8}
            }
    \end{center}
    \caption[VAE validation results, only distance loss and latent = 4]{VAE validation results over different amounts of joints and with a latent dimension of 4. Each experiment was conducted 10 times with different random seeds. The solid curve is the average over those 10 experiments and the color shaded area resembles the standard deviation. Notice that the y axis in bot plots is in log scale.}
    \label{fig:VAE_latent}
\end{figure}

In \figref{fig:VAE_latent} we have a closer look into the reconstruction loss and kl loss. While we increase the number of joints $N \in [2, 5, 10, 15]$ we also increase the level of complexity the network has to master to come up with suitable action which leads to a low distance loss. The increasing level of complexity is quite noticeable in both the kl loss and reconstruction loss. Turning to \figref{fig:VAE_latent} we can make following observations:\\
All KL-Loss curves in \figref{fig:VAE_latent} (d) to (e) are shaped in the same way. First a drop before rising with a little overshoot and finally approaching the terminal value asymptotically. While increasing $N$ we can see that the first local minimum shifts to the right, the upwards slope becomes slacker and the overshoot diminishes. This behavior confirms the the theory of increasing complexity while increasing the number of joints.\\
We can find another indicator by analyzing the reconstruction loss in \figref{fig:VAE_latent} (a) to (c). For all joints it is quite clear that the learning curves are shaped similar but approaching different final performances. For those experiments of $N \in [2, 5]$ we get a reconstruction loss $< 0.02$ independent of the latent dimension. But if we turn towards $N = 10$ we can clearly see that the latent dimension does make a significant difference between finding a solution, with a latent dimension of four, or failing with a latent dimension of eight. A further increase of $N$ does not lead to better reconstruction losses across all latent space dimensions. 
\begin{figure}[h]
    \begin{center}
        \subfloat[$N = 2$]{
            \includegraphics[width=0.23 \linewidth]{figures/experiments/vae_comparison_2_latent_2_4_8_reconstruction_loss.png}
            \label{fig:VAE_latent_comparison_reconstruction_loss/2}
            }
        \hfill
        \subfloat[$N = 5$]{
        \includegraphics[width=0.23 \linewidth]{figures/experiments/vae_comparison_5_latent_2_4_8_reconstruction_loss.png}
            \label{fig:VAE_latent_comparison_reconstruction_loss/5}
            }
        \hfill
        \subfloat[$N = 10$]{
            \includegraphics[width=0.23 \linewidth]{figures/experiments/vae_comparison_10_latent_2_4_8_reconstruction_loss.png}
            \label{fig:VAE_latent_comparison_reconstruction_loss/10}
            }
        \hfill
        \subfloat[$N = 15$]{
        \includegraphics[width=0.23 \linewidth]{figures/experiments/vae_comparison_15_latent_2_4_8_reconstruction_loss.png}
            \label{fig:VAE_latent_comparison_reconstruction_loss/15}
            }
    \end{center}
    \caption[VAE latent dimension comparison on reconstruction loss]{Comparison on the VAE latent dimension based on validation results over different amounts of joints. Each experiment was conducted 10 times with different random seeds. The solid curve is the average over those 10 experiments and the color shaded area resembles the standard deviation. Notice that the y axis in bot plots is in log scale. In the legend each label is structured as \texttt{<$N$>\textunderscore<latent dimension>}}
    \label{fig:VAE_latent_comparison_reconstruction_loss}
\end{figure}

By observing the individual number of joints in each plot of \figref{fig:VAE_latent_comparison_reconstruction_loss} form (a) to (d) we can observe that the latent dimension does make a difference in the final reconstruction loss performances. Starting with two joints there is no significant difference between the latent dimensions of two to eight. Continuing with 5 joints we can observe no significant difference in the finial performance but a slight difference with respect of how fast the models are converging. The experiments on a latent dimension of four and eight are very similar while the standard deviation for a latent dimension equal to two increases around 600 epochs indicating that some experiments have slight troubles to converge. \\
A significant difference in final performance can be observed with $N = 10$. Here the best performance is returned by experiments with a latent dimension of four closely followed by two. Comparing two and four in \figref{fig:VAE_latent_comparison_reconstruction_loss/10} shows that four is much more stable and faster in learning the problem as two. Last ranked are the experiments for a latent dimension of eight. Those experiments are by far not on the same level as with a latent dimension of two and four. A deeper look into the inference shows that the model fails to come up with an action to reach the given target position.\\
In \figref{fig:VAE_latent_comparison_reconstruction_loss/15} a performance difference is noticeable with a latent dimension of two as best and eight as worst. But since the best performance of the experiments with a latent dimension of two is on the same level as eight in \figref{fig:VAE_latent_comparison_reconstruction_loss/10} we can conclude that the the VAE fails at $N = 15$ with the present hyperparameter as in \chapref{chap:appendix}.

\subsubsection{Distance and Imitation Loss}


In this section, we present the outcomes of training the Variational Autoencoder (VAE) with the imitation loss using pre-calculated solutions from \algoref{alg:CCD} (CCD). An important configuration change involved setting \texttt{dataset target mode = ACTION} in the configuration file, instructing the dataset to calculate actions from CCD as well and setting the imitation loss weight to $0.01$.

Due to time constraints, we applied this loss function only in a setting with a latent dimension of four. This choice was based on the fact that a latent dimension of four had previously proven to be the best fit when using only the distance loss.

Overall, the distance loss functions depicted in \figref{fig:VAE_imitation/distance_loss} yield almost identical results to those obtained using the distance loss functions shown in \figref{fig:VAE_latent/reconstruction_4}. This similarity is evident in the rapid improvement of experiments with two and five joints but also in the same level increase in experiments with $N=10$ and $N=15$.

In addition to the distance loss, we also introduced an imitation loss between the computed action from CCD and the action generated by the neural network \figref{fig:VAE_imitation/imitation_loss}. Notably, the imitation loss increases with the parameter $N$, although not at the same scale. As $N$ grows, the differences between the imitation losses become smaller, but the convergence behavior where the curves experience a significant drop at the beginning of training and then stabilize, seemingly reaching their optimal values stays constant over all different $N$.

Finally, when examining the KL-divergence in \figref{fig:VAE_imitation/kl_loss}, we observe an almost indistinguishable pattern compared to \figref{fig:VAE_latent/kl_4}.

\begin{figure}[h]
    \begin{center}
        \subfloat[Distance loss]{
        \includegraphics[width=0.31 \linewidth]{figures/experiments/vae_comparison_imitation_[2, 5, 10, 15]_latent_4 4 4 4_distance_loss.png}
            \label{fig:VAE_imitation/distance_loss}
            }
        \hfill
        \subfloat[Imitation loss]{
        \includegraphics[width=0.31 \linewidth]{figures/experiments/vae_comparison_imitation_[2, 5, 10, 15]_latent_4 4 4 4_imitation_loss.png}
            \label{fig:VAE_imitation/imitation_loss}
            }
        \hfill
        \subfloat[KL diveregence from standard normal distribution]{
        \includegraphics[width=0.31 \linewidth]{figures/experiments/vae_comparison_imitation_[2, 5, 10, 15]_latent_4 4 4 4_kl_loss.png}
            \label{fig:VAE_imitation/kl_loss}
            }
    \end{center}
    \caption[VAE validation results with imitation loss]{Results on the validation dataset while training a VAE with a latent dimension of 4 but incorporating an imitation loss weighted with 0.01. Results are collected from ten different runs and with a solid mean curve and a shaded standard deviation area. $N \in [2, 5, 10, 15]$} 
    \label{fig:VAE_imitation}
\end{figure}



\subsection{Supervised}

For actions computed by the supervised model we only relied on state information as the input to predict an action to reach from the current state the given target position.  Since this approach is very similar to the Variational Autoencoder setting we are also able to apply the afore mentioned Inverse Kinematics Loss with distance and imitation loss as in \eqref{eqn:IK-Loss}.

The results of the supervised experiments are divided into two parts. The first is only applying the distance loss while the second one is incorporating also an imitation loss with respect ot pre computed actions from the CCD solver.

\subsubsection{Distance Loss}

The distance loss results over different amount of joints in \figref{fig:supervised_distance} are very similar compared to \figref{fig:VAE_latent} (a) to (c). Parallel to \figref{fig:VAE_latent} we can also observe an overall increase in distance loss and finishing at different convergence levels while increasing $N$. Different to the experiments on the VAE is the standard deviation between the experiments. The supervised experiments show a significant smaller standard deviation across the whole training period compared to the VAE experiments.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.46 \linewidth]{figures/experiments/supervised_2_distance_loss.png}
    \end{center}
    \caption[Supervised Distance Loss]{Distance loss for supervised experiments over different $N \in [2, 5, 10, 15, 20]$. For each $N$ we conducted 10 experiments. }
    \label{fig:supervised_distance}
\end{figure}

\subsubsection{Distance and Imitation Loss}

For this series of experiments we also want to emphasis solutions close to solutions computed by the CCD by setting the dataset mode to \texttt{ACTION} and setting the imitation loss weight to $0.01$. 

Unlike as in the previous section the distance loss differs a lot from \figref{fig:supervised_distance}. In \figref{fig:supervised_distance} we can all curves start at almost the same loss value but the metric curves in \figref{fig:VAE_imitation/distance_loss} are verticaly shifted and approaching their final value very fast and with a lot less variance between the experiements with the same $N$.\\
A bit more interesting are the results on the imitation loss. Similar to \figref{fig:VAE_imitation/imitation_loss} all curves have a drop in the first couple of epochs but are constant for the rest of the training. Comparing those curves with respect to the number of joints we observed that all mean curves have vastly different levels of convergence which seems not to be linear correlated with $N$.


In this series of experiments, our primary focus is on generating solutions that closely align with those computed by the Cyclical Coordinate Descent (CCD) method. To achieve this, we configured the dataset mode as \texttt{ACTION} and set a low imitation loss weight of $0.01$.

In contrast to the previous section, the distance loss exhibits substantial differences from what we observed in \figref{fig:supervised_distance}. Unlike the curves in \figref{fig:supervised_distance}, where they all start with nearly identical loss values and need many epochs to converge, the metric curves in \figref{fig:VAE_imitation/distance_loss} exhibit vertical shifts and converge to their final values rapidly, with considerably less variability among experiments sharing the same $N$.

More intriguing are the outcomes related to the imitation loss. Much like what is depicted in \figref{fig:VAE_imitation/imitation_loss}, all curves experience a drop in the initial epochs and subsequently maintain a relatively stable value throughout the rest of the training. Upon comparing these curves in relation to the number of joints, we have observed that the mean curves exhibit notably distinct convergence levels, and these do not appear to follow a linear correlation with $N$.


\begin{figure}[h]
    \begin{center}
        \subfloat[Distance Loss]{
            \includegraphics[width=0.46 \linewidth]{figures/experiments/supervised_imitation_2_latent_4 4 4 4_distance_loss.png}
            \label{fig:supervised_imitation/distance_loss}
            }
        \hfill
        \subfloat[Imitation Loss]{
        \includegraphics[width=0.46 \linewidth]{figures/experiments/supervised_imitation_2_latent_4 4 4 4_imitation_loss.png}
            \label{fig:supervised_imitation/imitation_loss}
            }
    \end{center}
    \caption[Supervised Distance and Imitation Loss]{Validation results for supervised experiments on distance and imitation loss. }
    \label{fig:supervised_imitation}
\end{figure}

\section{Reinforcement Learning}

In this section we will present results on the reinforcement learning experiments on the 2D inverse kinematics environment. \\
First we will set a benchmark with the plane Soft Actor-Critic. Further we will combine Soft Actor-Critic with pre-trained latent models and have a look into their performance statistics but also comparing it to the plane Soft Actor-Critic results. 

\subsection{Baseline Soft Actor-Critic}

First we would like to have a look how a the plain SAC algorithm, where the actor network directly calculates actions for the environment, performs on the inverse kinematics environment as described in \secref{sec:RL-Environment}. Further we will describe this kind of experiments as SAC baseline experiments. All Hyperparameters used for the SAC baseline experiments can be found in \chapref{chap:appendix}.
\begin{figure}[h]
    \begin{center}
        \subfloat[Mean reward per step over the last 20 episodes.]{
            \includegraphics[width=0.46 \linewidth]{figures/experiments/sac_baseline_mean_score.png}
            \label{fig:SAC_baseline/reward}
            }
        \hfill
        \subfloat[Mean episode length over the last 20 episodes]{
        \includegraphics[width=0.46 \linewidth]{figures/experiments/sac_baseline_episode_len.png}
            \label{fig:SAC_baseline/episode_len}
            }
    \end{center}
    \caption[SAC baseline experiment results]{SAC baseline experiment results with an increasing number of joints. We can clearly see that the algorithm does not scale well with an increasing number of joints and therefor drops in performance. Each experiment was conducted 10 times and the shaded areas resemble the standard deviation around the mean.}
    \label{fig:SAC_baseline}
\end{figure}

In \figref{fig:SAC_baseline} we plotted the mean reward per step and the average episode length over the last 20 epochs for different $N \in [2, 5, 10, 15, 20]$. Overall we can observe a decrease of collected reward per step in \figref{fig:SAC_baseline/reward} and an increasing amount of steps in one episode in \figref{fig:SAC_baseline/episode_len} while increasing the number of joints $N$. In \figref{fig:SAC_baseline/episode_len} the training curves are approaching the maximum step limit of one episode while increasing the number of joints. Additionally we can observe that the standard deviation over multiple experiments is decreasing while increasing the number of joints.

To show the actual behavior of an agent in a standardized setting we also plot an inference episode in \figref{fig:SAC_baseline_inference_trajectory} and the distance remaining to target in \figref{fig:SAC_baseline_inference_distance}. The standardization in \figref{fig:SAC_baseline_inference_trajectory} for one selected experiment for a relative consistent target position at $[-0.5, 0.5] \cdot N$. \\
As expected from the performance statistics in \figref{fig:SAC_baseline} agents with two and five joints perform much better than agents with ten or more joints. \\
Interesting to observe is that all agents are minimizing the distance to the target quite fast in their first couple of steps. But after missing the target closely the agents behavior starts to oscillate.
For closer inspection please refer to \figref{fig:SAC_baseline_inference_trajectory/greedy_15} and \figref{fig:SAC_baseline_inference_trajectory/greedy_20}. \\


\begin{figure}[h]
    \begin{center}
        \subfloat[From experiment 2\textunderscore 1691621262.]{
            \includegraphics[width=0.31 \linewidth]{figures/experiments/greedy_inference_baseline_2_1691621262_5000.png}
            \label{fig:SAC_baseline_inference_trajectory/greedy_2}
            }
        \hfill
        \subfloat[From experiment 5\textunderscore 1691624939. To make things more clear only every 5th robot arm is drawn]{
            \includegraphics[width=0.31 \linewidth]{figures/experiments/greedy_inference_baseline_5_1691624939_5000.png}
            \label{fig:SAC_baseline_inference_trajectory/greedy_5}
            }
        \hfill
        \subfloat[From experiment 10\textunderscore 1691622498. To make things more clear only every 5th robot arm is drawn]{
            \includegraphics[width=0.31 \linewidth]{figures/experiments/greedy_inference_baseline_10_1691622498_5000.png}
            \label{fig:SAC_baseline_inference_trajectory/greedy_10}
            }
        \\
        \subfloat[From experiment 15\textunderscore 1691619106. To make things more clear only every 40th robot arm is drawn]{
            \includegraphics[width=0.31 \linewidth]{figures/experiments/greedy_inference_baseline_15_1691619106_5000.png}
            \label{fig:SAC_baseline_inference_trajectory/greedy_15}
            }
        \subfloat[From experiment 20\textunderscore 1691619159. To make things more clear only every 40th robot arm is drawn]{
            \includegraphics[width=0.31 \linewidth]{figures/experiments/greedy_inference_baseline_20_1691619159_5000.png}
            \label{fig:SAC_baseline_inference_trajectory/greedy_20}
            }
    \end{center}
    \caption[SAC baseline inference]{SAC inference. The trajectory is plotted in red. robot arms are drawn in yellow with the little dots as positions of each joint. Target position and start position are scattered in blue and green. The space an end-effector is able to reach is plotted in grey.}
    \label{fig:SAC_baseline_inference_trajectory}
\end{figure}
\begin{figure}[h]
    \begin{center}
        \subfloat[Experiment 2\textunderscore 1691621262.]{
        \includegraphics[width=0.31 \linewidth]{figures/experiments/Distance_to_target_baseline_2_1691621262_5000.png}
            \label{fig:SAC_baseline_inference/distance_2}
            }
        \hfill
        \subfloat[Experiment 5\textunderscore 1691624939 ]{
        \includegraphics[width=0.31 \linewidth]{figures/experiments/Distance_to_target_baseline_5_1691624939_5000.png}
            \label{fig:SAC_baseline_inference/distance_5}
            }
        \hfill
        \subfloat[Experiment 10\textunderscore 1691622498 ]{
        \includegraphics[width=0.31 \linewidth]{figures/experiments/Distance_to_target_baseline_10_1691622498_5000.png}
            \label{fig:SAC_baseline_inference/distance_10}
            }
        \\
        \subfloat[Experiment 15\textunderscore 1691619106 ]{
        \includegraphics[width=0.31 \linewidth]{figures/experiments/Distance_to_target_baseline_15_1691619106_5000.png}
            \label{fig:SAC_baseline_inference/distance_15}
            }
        \subfloat[Experiment 20\textunderscore 1691619159 ]{
        \includegraphics[width=0.31 \linewidth]{figures/experiments/Distance_to_target_baseline_20_1691619159_5000.png}
            \label{fig:SAC_baseline_inference/distance_20}
            }
    \end{center}
    \caption[SAC baseline inference]{Distance to target for each action outcome. The target is consistent at $[-0.5, 0.5] \cdot N$. The threshold of 0.1 at which an episode is concluded successfully is drawn in red.} 
    \label{fig:SAC_baseline_inference_distance}
\end{figure}
Another noteworthy observation is the agent's approach in reaching the target position. Rather than taking a direct path towards the target, the agent tends to move its end-effector to the outer boundary of its reach before descending towards the target. This behavior warrants further discussion.

\subsection{Soft Actor-Critic and Variational Autoencoder}

In this section we are going to show the experimental results of combining a Variational Autoencoder with Soft Actor-Critic. As we know from \chapref{chap:Methodology} we only use the decoder of the trained Variational Autoencoder with input from the corresponding state and a latent action directly sampled from the parameterized distribution from the actor network.

\begin{table}
    \begin{center}
        \begin{tabular}{ l | c  c | c  c | c  c}
        \textbf{$N$} & \multicolumn{6}{c}{latent dimension} \\
        \hline
        & \multicolumn{2}{c |}{2} & \multicolumn{2}{c |}{4} & \multicolumn{2}{c}{8} \\
        & random seed & epoch & random seed & epoch & random seed & epoch \\
        \hline
        2   & 1693105015  & 4950 & 1692461245   & 4890 & 1691608881   & 1480 \\
        5   & 1693096269  & 4495 & 1692476369   & 4795 & 1691606505   & 1290 \\
        10  & 1693098415  & 3615 & 1691627455   & 2535 & 1691608513   & 2485  \\
        15  & 1693103204  & 4815 & 1691628295   & 3845 & 1691618374   & 4760  \\
        \end{tabular}
    \end{center}
    \caption[Used VAE checkpoints for SAC]{Used checkpoints to train SAC with decoder from VAE. All experiments can be found in \texttt{results/vae/<$N$>\textunderscore<latent dimension>\textunderscore <random seed>/VAE\textunderscore<epoch>*.pt}}
    \label{tab:VAE_checkpoints_SAC}
\end{table}

As we have seen in the previous section we carried out VAE experiments with three different latent space dimensions. Accordingly we trained ten SAC runs for each latent space size and number of joints $N \in [2, 5, 10, 15]$. We used the best VAE checkpoint available according to the overall validation loss. A detailed listing which checkpoints where actually used can be found in \tabref{tab:VAE_checkpoints_SAC}. 

\begin{figure}[h]
    \begin{center}
        \subfloat[mean episode reward per step averaged over the last 20 episodes. With a \textbf{latent dimension = 2}]{
            \includegraphics[width=0.31 \linewidth]{figures/experiments/sac_latent_actor_2_mean_score.png}
            \label{fig:SAC_latent_4/reward}
            }
        \hfill
        \subfloat[mean episode reward per step averaged over the last 20 episodes. With a \textbf{latent dimension = 4}]{
            \includegraphics[width=0.31 \linewidth]{figures/experiments/sac_latent_actor_4_mean_score.png}
            \label{fig:SAC_latent_4/reward}
            }
        \hfill
        \subfloat[mean episode reward per step averaged over the last 20 episodes. With a \textbf{latent dimension = 8}]{
            \includegraphics[width=0.31 \linewidth]{figures/experiments/sac_latent_actor_8_mean_score.png}
            \label{fig:SAC_latent_8/reward}
            }
        \\
        \subfloat[mean episode length avergaed over the last 20 episodes. With a \textbf{latent dimension = 2}]{
            \includegraphics[width=0.31 \linewidth]{figures/experiments/sac_latent_actor_4_episode_len.png}
            \label{fig:SAC_latent_4/episode_len}
            }
        \hfill
        \subfloat[mean episode length avergaed over the last 20 episodes. With a \textbf{latent dimension = 4}]{
        \includegraphics[width=0.31 \linewidth]{figures/experiments/sac_latent_actor_4_episode_len.png}
            \label{fig:SAC_latent_4/episode_len}
            }
        \hfill
        \subfloat[mean episode length avergaed over the last 20 episodes. With a \textbf{latent dimension = 8}]{
        \includegraphics[width=0.31 \linewidth]{figures/experiments/sac_latent_actor_8_episode_len.png}
            \label{fig:SAC_latent_8/episode_len}
            }
    \end{center}
    \caption[SAC + VAE on latent dim = 4]{SAC + decoder form VAE with different latent dimensions. Each experiment was conducted 10 times with different random seeds. The solid strong line is the mean over those 10 experiments. The color shaded area covering the mean is the standard deviation around the mean.}
    \label{fig:SAC_latent_4}
\end{figure}

\begin{figure}[h]
    \begin{center}
        \subfloat[$N = 2$]{
            \includegraphics[width=0.23 \linewidth]{figures/experiments/sac_latent_comp_2_vae_mean_score.png}
            \label{fig:SAC_latent_comparison_mean_score/2}
            }
        \hfill
        \subfloat[$N = 5$]{
        \includegraphics[width=0.23 \linewidth]{figures/experiments/sac_latent_comp_5_vae_mean_score.png}
            \label{fig:SAC_latent_comparison_mean_score/5}
            }
        \hfill    
        \subfloat[$N = 10$]{
            \includegraphics[width=0.23 \linewidth]{figures/experiments/sac_latent_comp_10_vae_mean_score.png}
            \label{fig:SAC_latent_comparison_mean_score/10}
            }
        \hfill
        \subfloat[$N = 15$]{
        \includegraphics[width=0.23 \linewidth]{figures/experiments/sac_latent_comp_15_vae_mean_score.png}
            \label{fig:SAC_latent_comparison_mean_score/15}}
    \end{center}
    \caption[SAC + VAE latent dimension comparison mean score]{Shown are the collected mean scores over the the last 20 episodes per step, compared different latent dimensions and the baseline. Solid line is the mean over 10 experiments and the color shaded area is the corresponding standard deviation. Higher means better with 0 as the maximum. The colored dotted lines mark an end of an experiment.}
    \label{fig:SAC_latent_comparison_mean_score}
\end{figure}

In \figref{fig:SAC_latent_comparison_mean_score} we can observe what difference the choice of latent dimension has on the performance of the Soft Actor Critic algorithm. Quite obviously to see is that the choice of latent dimension really starts to matter after five joints. Where at $N=10$ a latent dimension of two and four succeed but eight doesn't. This is clearly related to the performance of the VAE during its training. There we were also able to see that the latent dimension has an impact of the reconstruction loss performance of the VAE. Therefor it is no surprise to see that the best VAE checkpoints on 15 joints and the best VAE checkpoint on ten joints with a latent dimension of eight fails for the SAC + VAE setting. 

By having a closer look into the individual plots of \figref{fig:SAC_latent_comparison_mean_score} we can difference between the collected mean score. Starting at two joints all mean curves are packet closely together where at five joints we can start to see a slight decrease in performance at 1000 epochs continuing up to 5000 epochs, between the experiments done with a latent dimension of two and the others.\\
This decrease is even more noticeable at ten joints where is now a gap between the experiments done with a latent dimension of four and the baseline experiments too.
\begin{figure}[h]
    \begin{center}
        \subfloat[$N = 2$]{
            \includegraphics[width=0.23 \linewidth]{figures/experiments/sac_latent_comp_2_vae_episode_len.png}
            \label{fig:SAC_latent_comparison_episode_len/2}
            }
        \hfill
        \subfloat[$N = 5$]{
        \includegraphics[width=0.23 \linewidth]{figures/experiments/sac_latent_comp_5_vae_episode_len.png}
            \label{fig:SAC_latent_comparison_episode_len/5}
            }
        \hfill    
        \subfloat[$N = 10$]{
            \includegraphics[width=0.23 \linewidth]{figures/experiments/sac_latent_comp_10_vae_episode_len.png}
            \label{fig:SAC_latent_comparison_episode_len/10}
            }
        \hfill
        \subfloat[$N = 15$]{
        \includegraphics[width=0.23 \linewidth]{figures/experiments/sac_latent_comp_15_vae_episode_len.png}
            \label{fig:SAC_latent_comparison_episode_len/15}
            }
    \end{center}
    \caption[SAC + VAE latent dimension comparison episode length]{Shown are the mean episode length over the the last 20 episodes, compared different latent dimensions and the baseline. Solid line is the mean over 10 experiments and the color shaded area is the corresponding standard deviation. Lower means better with 1 as the minimum and 400 as the maximum.}
    \label{fig:SAC_latent_comparison_episode_len}
\end{figure}

In \figref{fig:SAC_latent_comparison_episode_len} we can observe the mean episode length for different $N \in [2, 5, 10, 15]$. Starting at two joints we can see not much of a difference but at five joints we can observe slight differences between different latent dimensions. First up is the latent dimension of two At this value the experiments perform as in \figref{fig:SAC_latent_comparison_mean_score/5} worst.\\
Moving on to a latent dimension of four, those experiments perform best in terms of mean episode length and even undercut the baseline experiments although they have not performed as good in terms of the mean score per step in \figref{fig:SAC_latent_comparison_mean_score/5}. Finally those experiments with a latent dimension of eight are performing almost on the same level as the baseline experiments but not as good as the experiments with a latent dimension of four. \\
In \figref{fig:SAC_latent_comparison_mean_score/10} we can see wide variety of best performances. The best performing setting are the baseline experiments followed by the experiments with a latent dimension of four. The next best performance is coming from a latent dimension equal to two and last ranked are the experiments with a latent dimension of eight. As we have mentioned earlier we suspect this drop in performance caused by the quality the decoder gets hand over to the Soft Actor Critic. \\
Note that although the maximum number of steps per episode was set to 400 the standard deviation of the experiment with a latent dimension of eight seam to surpass this threshold which is with a closer look into the actual curves clearly not the case. \\ 
On the performed experiments with 15 joints we can see that none of the latent experiments is able to match the performance of the baseline experiments with even a couple of experiments with a latent dimension of four or eight aborting early. As it turns out this abort is caused by abnormally high values for alpha loss as shown in \figref{fig:alpha_loss_15} resulting in \texttt{nan} values in update steps. A positive alpha loss translates to a an increase of alpha translates to an increase in exploration for the policy, by encouraging a higher standard deviation for the parameterized distribution returned by the actor network.
\begin{figure}[h]
    \begin{center}
        \subfloat[mean episode reward per step averaged over the last 20 episodes. ]{
            \includegraphics[width=0.46 \linewidth]{figures/experiments/sac_vae_imitation_[2, 5, 10, 15]_mean_score.png}
            \label{fig:SAC_latent_8/reward}
            }
        \hfill
        \subfloat[mean episode length avergaed over the last 20 episodes]{
        \includegraphics[width=0.46 \linewidth]{figures/experiments/sac_vae_imitation_[2, 5, 10, 15]_episode_len.png}
            \label{fig:SAC_latent_8/episode_len}
            }
    \end{center}
    \caption[SAC + VAE on latent dim = 4]{SAC + decoder form VAE with \textbf{latent dimension = 4} and trained with imitation loss enabled. Each experiment was conducted 10 times with different random seeds. The solid strong line is the mean over those 10 experiments. The color shaded area covering the mean is the standard deviation around the mean.}
    \label{fig:SAC_latent_8}
\end{figure}

\begin{figure}[h]
    \begin{center}
        \subfloat[SAC baseline experiments]{
            \includegraphics[width=0.31 \linewidth]{figures/experiments/action_correlations_baseline_2_1691621262_5000.png}
            \label{fig:SAC_action_correlation_comp/baseline}
            }
        \hfill
        \subfloat[SAC + VAE with latent dimension = 4 and only distance loss enabled]{
            \includegraphics[width=0.31 \linewidth]{figures/experiments/action_correlations_latent_actor_4_2_1693063115_5000.png}
            \label{fig:SAC_action_correlation_comp/latent_4}
            }
        \subfloat[SAC + VAE with latent dimension = 4, distance and imitation loss enabled]{
            \includegraphics[width=0.31 \linewidth]{figures/experiments/action_correlations_latent_imitation_4_2_1693269830_3000.png}
            \label{fig:SAC_action_correlation_comp/latent_4_imitation}
            }
    \end{center}
    \caption[action correlation comparison]{Action correlation for baseline experiments, latent dimension only trained on distance loss and SAC + VAE trained on distance and imitation loss. }
    \label{fig:SAC_action_correlation_comp}
\end{figure}

Because we have seen a lack in performance between the baseline SAC experiments we additional trained VAEs with the imitation loss enabled to minimize the distance between the state angle distribution the decoder is receiving as in \figref{fig:SAC_action_correlation_comp} versus the state angle distribution the decoder is trained as in \figref{fig:dataset_action_correlation}. But if we have a look into \figref{fig:SAC_action_correlation_comp/latent_4_imitation} and compare to the plots on the left we do not see a significant change. This could be extrapolated to higher $N$ and explain the missing improvement we would expect. But since this is not the main focus of this thesis we leaf these steps for future research.

\subsection{Soft Actor-Critic and Supervised}

\begin{figure}[h]
    \begin{center}
        \subfloat[Mean score per step]{
            \includegraphics[width=0.46 \linewidth]{figures/experiments/sac_supervised_dist_[2, 5, 10]_mean_score.png}
            \label{fig:sac_supervised/mean_score}
            }
        \hfill
        \subfloat[episode length]{
        \includegraphics[width=0.46 \linewidth]{figures/experiments/sac_supervised_dist_[2, 5, 10]_episode_len.png}
            \label{fig:sac_supervised/episode_len}
            }
    \end{center}
    \caption[SAC + Supervised Distance Loss]{Validation results for supervised experiments only on distance loss}
    \label{fig:sac_supervised}
\end{figure}

Merging the supervised model with SAC yields performances as in \figref{fig:sac_supervised}. We do can see very similar performances as with merging SAC with VAE. Starting with the similarities the achieved mean scores achieved from the SAC + VAE model in \figref{fig:SAC_latent_comparison_mean_score} are very close to those achieved by SAC + supervised in \figref{fig:sac_supervised/mean_score}. Also the curve shapes are similar but the achieved episode lenghts are significant higher for SAC + supervised (factor $\approx 2$).

Combining the supervised model with SAC produces performance results similar to those depicted in \figref{fig:sac_supervised}. Notably, the mean scores achieved by the SAC + VAE model in  \figref{fig:SAC_latent_comparison_mean_score} closely resemble those attained by SAC + supervised in \figref{fig:sac_supervised/mean_score}. Furthermore, the overall curve shapes exhibit similarities, but it's worth highlighting that the episode lengths achieved by SAC + supervised are notably higher, approximately a factor of 2.


\begin{table}
    \begin{center}
        \begin{tabular}{ l | c  c | c  c | c  c | c c }
        experiment series & \multicolumn{6}{c}{$N$} \\
        \hline
        & \multicolumn{2}{c |}{2} & \multicolumn{2}{c |}{5} & \multicolumn{2}{c}{10} & \multicolumn{2}{c}{15}\\
        & $\mu$ & $\sigma$ & $\mu$ & $\sigma$ & $\mu$ & $\sigma$ & $\mu$ & $\sigma$\\
        \hline
        \textit{baseline}                    &  -5.2 & 1.0 & -15.2 & 1.6 & -32.5 & 2.6 & -47.8 & 3.6   \\
        \textit{latent actor 4}              & -13.5 & 2.9 & -15.6 & 2.5 & -15.5 & 4.4 &  32.8 & 102.4 \\
        \textit{latent imitation 4}          & -12.6 & 4.0 & -12.4 & 1.5 & -13.2 & 0.8 & -23.2 & 3.3   \\
        \textit{supervised distance loss}    &  -9.0 & 1.0 & -10.5 & 0.4 & -10.9 & 1.0 &       &       \\
        \textit{supervised imitation loss}   &  11.0 & 2.1 &  41.8 & 2.8 &  39.7 & 2.9 &       &       \\
        \end{tabular}
    \end{center}
    \caption[policy log probabilities]{Log probabilities of each policy series during training form the last 50 episodes each over 10 experiments. }
    \label{tab:policy_log_probs}
\end{table}

Another distinguishing factor when comparing the fusion of SAC with either the VAE or the supervised regressor approach can be observed in the distribution of log probabilities returned by the actor networks for sampling from the Gaussian distribution, see \tabref{tab:policy_log_probs}
.

For the \textit{baseline} experiments, the mean log probability decreases as the number of joints increases, indicating an increase in policy stochasticity. In contrast, experiments with latent models \textit{latent actor 4} and  \textit{latent imitation 4} maintain almost consistent log probabilities across different values of $N$. However, experiments involving SAC + supervised models exhibit substantially higher log probabilities. Further discussion on this observation will be provided in \chapref{chap:discussion}.

\begin{figure}[h]
    \begin{center}
        \subfloat[Mean score per step]{
            \includegraphics[width=0.46 \linewidth]{figures/experiments/sac_supervised_imitation[2, 5, 10]_mean_score.png}
            \label{fig:sac_supervised_imitation/distance_loss}
            }
        \hfill
        \subfloat[episode length]{
        \includegraphics[width=0.46 \linewidth]{figures/experiments/sac_supervised_imitation[2, 5, 10]_episode_len.png}
            \label{fig:sac_supervised_imitation/imitation_loss}
            }
    \end{center}
    \caption[SAC + Supervised Distance and Imitation Loss]{Validation results for supervised experiments on distance and imitation loss. }
    \label{fig:sac_supervised_imitation}
\end{figure}

Lastly, experiments conducted with a supervised model trained using imitation loss are depicted in \figref{fig:sac_supervised_imitation}. It's evident that all experiments in this category do not reach the same level of performance as the baseline experiments or other combinations of SAC with latent models.