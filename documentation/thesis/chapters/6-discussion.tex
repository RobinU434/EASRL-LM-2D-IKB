\chapter{Discussion}\label{chap:discussion}

In this chapter we are going to discuss the experiments presented in \chapref{chap:experiments}.

\section{Baseline SAC}

Starting with the baseline results for SAC we have seen a clear correlation between the algorithms performance and the number of joints we are trying to train on. This is reasonable because also other more standard Inverse Kinematics solver, including CCD are highly dependent on the number of joints required to solve for, as we have seen in the section about runtime complexity of CCD. 
While comparing the CCD with baseline SAC we found a couple of differences those two solvers present.
\begin{itemize}
    \item \textbf{Target Dependence}: By looking deeper into CCD it is clear that its granular performance in a single run is also highly dependent on the given start configuration and target. \todo{plot circular heatmap with constant start position and mutliple target postions with each point has the number of iterations needed to reach the target with an accuracy of 0.1} Note that we talk about the performance of CCD as the number of iterations needed to reach the desired target position. If we now look into the steps needed to reach the desired target position of SAC we can seed that while increasing the number of joints we are increasingly often capped by the maximum number of iterations than ending the episode by reaching the target. On possible reason is teased by \figref{} \todo{ref to baseline SAC inference}. Here we were able to see that the agents behavior sometimes collapses as it is about to reach the target but after failing very closely the end-effector moves rapidly towards the origin and never recovers as close as before. One reason why the agent struggles to reach the target with a creasing amount of joints could be the constant threshold to end an episode of 0.1 euclidean distance between end-effector and target position. As we increase the number of joints the area where the end-effector position ends an episode decreases due to $A = \pi N^2$ quadratically which makes it even harder to reach the position while increasing $N$. Against this argument stands that the robot arm is capable to perform more precise movements or even reducing the problem further down. On strategy could position joint 1 to $N - 2$ so the last two joints are able to reach the target. 
    \item \textbf{Solutions}: While having a deeper look into the solutions the SAC and CCD find to solve Inverse Kinematics we can observe a vast difference in the state angle distribution \todo{add plot of state angle distribution of a sac baseline experiment with 2 joints}. While in case of CCD the states to reach the target distribution are rhombus shaped the state angle plot in \figref{} is way more scattered. This change is also observable while looking deeper into the chosen trajectory. 
\end{itemize}

Note that this is only an qualitative comparison because CCD has the full action space on its disposal while SAC is limited to -1 to one because of the action normalizer at the back of SAC. Scale the tanh output up to a span of $2pi$ or higher is not the perfect solution because SAC could find multiple actions for the same action outcome. It could also interfere with computing the probability of $\log(\pi_\theta(a|s))$.   

The presented decrease in the SAC performance in \figref{fig:SAC_baseline} could be also explained because of the linear increase of complexity of the observation space since $S \subset \mathbb{R}^{4+N}$. To counter the increase of complexity some could also increase the number of hidden layer or neurons per layer and have a closer look into neural architecture search. Another way to maybe proof this theory could be the application of Neural-Evolution-of-Augmented-Topologies short NEAT. This algorithm finds the smallest possible architecture to conquer a given task. Given the provided background information we assume the found neural architecture increases in complexity while increasing $N$. 

\section{Latent Model}

Moving on to the experiments to train a latent model. We train a latent model to augment SAC and let it control the latent model and therefor the actions from a higher level while having access to all state space information.

Since the standalone results of each latent model type are very similar I will discuss them together.
Both ways to train a model with the TargetGaussian Dataset yielded almost the same performance in the distance loss. This is not very surprising as we have seen neural networks struggle with the inverse Kinematics task while increasing the number of joints $N$. Looking deeper into the model performance shows that the distance loss is dependent is also dependent on the state information we are providing in datasets. \todo{think about how to plot this}   

We also have similar results with the imitation loss
\todo{big change in observation space distribution}


\section{SAC with Latent Model}

Embedding the Latent Model into the workflow of SAC yielded performances as presented in \chapref{chap:experiments}. While understanding and seeing the vast difference in problem solving strategies between the trained latent models and the Inverse Kinematics solver CCD it is quite surprising this approach works in the first place. Usually such a significant change in the observation space distribution, leads to unpredictable and unfeasable behavior in the predicted actions in the Action space. We tried to master this issue by introducing the imitation loss to emphasis predicted actions leading to state angles which are much closer to the original distribution. \todo{look into results.} Arguments that the approach actually works can be found by looking into a greedy inference with the latent models where we simulate a very light weight version of the Plane-Robot-Environment while assuming the optimal policy. 