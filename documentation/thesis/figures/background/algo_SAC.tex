\begin{algorithm}[p]
\caption{Soft Actor Critic}\label{alg:SAC}
\begin{algorithmic}
    \State{} Input: initial policy parameters $\theta$, Q-function parameters $\phi_0$, $\phi_1$, empty replay buffer $\mathcal{D}$
        \State{} Set target parameters equal to main parameters $\phi_{\text{target}, 0}$ $\leftarrow$ $\phi_0$, $\phi_{\text{target}, 1}$ $\leftarrow$ $\phi_1$ 
        \For{$i$ in number of epochs}
        \State{} $s$ $\leftarrow$ reset environment
        
        \For{$t$ number of timesteps}
        \State{} $a$ $\sim$ $\pi_\theta(\cdot | s)$
        \State{} $s'$, $r$, $d$ $\leftarrow$ execute $a$ in environment
        \State{} Store $(s, a, r, s', d)$ in replay buffer $\mathcal{D}$
        \If{$d$ is true}
        \State{} break and reset environment
        \EndIf{}
        \State{} $s$ $\leftarrow$ $s'$
        \EndFor{}
        
        \If{$|\mathcal{D}|$ > minimal buffer size}
        \For{number of train iterations}
        \State{} sample minibatch: $(s_\mathcal{B}, a_\mathcal{B}, r_\mathcal{B}, s'_\mathcal{B}, d_\mathcal{B}) = \mathcal{B}$ $\leftarrow$ $\mathcal{D}$ 
        \State{} compute td target with $\tilde{a}'_\mathcal{B} \sim \pi_\theta(\cdot|s'_\mathcal{B})$
        \begin{equation*}
            td(r_\mathcal{B}, s'_\mathcal{B}, d_\mathcal{B}) = r + \gamma \cdot d \cdot \left(\min_{i\in\{0, 1\}}\left(Q_{\phi_{\text{target}, i}}(s'_\mathcal{B}, \tilde{a}'_\mathcal{B})\right) - \alpha \cdot \log \pi_\theta(\tilde{a}'_\mathcal{B}|s'_\mathcal{B})\right)
        \end{equation*}
        \State{} Update Q-functions for parameters $\phi_i$ $i \in \{0, 1\}$ using:
        \begin{equation*}
            \nabla_{\phi_i} \frac{1}{|B|} \sum_{k \in |\mathcal{B}|}\mathcal{L}_\beta\left(td(r_{\mathcal{B}, k}, s'_{\mathcal{B}, k}, d_{\mathcal{B}, k}), Q_{\phi_i}(s_{\mathcal{B}, k}, a_{\mathcal{B}, k})\right)    
        \end{equation*}
        \State{} Update policy with $\tilde{a}_\mathcal{B} \sim \pi_\theta(\cdot|s_\mathcal{B})$ using:
        \begin{equation*}
           \nabla_{\theta} \frac{1}{|\mathcal{B}|}\sum_{k \in |\mathcal{B}|}\min_{i\in\{0, 1\}}Q_{\phi_i}(s_{\mathcal{B}, k}, \tilde{a}_{\mathcal{B}, k}) - \alpha \cdot \log \pi_\theta(\tilde{a}_{\mathcal{B}, k}|s_{\mathcal{B}, k})
        \end{equation*}
        \State{} Update $\alpha$ with target entropy $H_\text{target}$ and $\tilde{a}_\mathcal{B} \sim \pi_\theta(\cdot|s_\mathcal{B})$ using:
        \begin{equation*}
            \nabla_\alpha -\frac{\alpha}{|\mathcal{B}|} \sum_{k \in |\mathcal{B}|} \left(\log \pi_\theta(a'_{\mathcal{B}, k}|s_{\mathcal{B}, k}) - H_\text{target}\right)
        \end{equation*}
        \EndFor{}
        \EndIf{}
        \EndFor{}
\end{algorithmic}
\end{algorithm}